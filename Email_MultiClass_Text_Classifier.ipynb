{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files (x86)\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tifImageFilePath = 'D:\\\\Temp\\\\TrainingData\\\\'\n",
    "jpgImageFilePath = 'D:\\\\Temp\\\\Final_JPEG\\\\'\n",
    "tifTestImageFilePath = 'D:\\\\Temp\\\\TestingData\\\\'\n",
    "jpgTestImageFilePath = 'D:\\\\Temp\\\\Final_Testing_JPEG\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert All tif files to jpeg\n",
    "def ConvertTifToJpeg(tifImageFilePath, jpgImageFilePath):\n",
    "    for r, d, f in os.walk(tifImageFilePath):\n",
    "        for file in f:\n",
    "            if ('.tiff' in str.lower(file)) | ('.tif' in str.lower(file)):\n",
    "                tifImage = Image.open(tifImageFilePath + file)\n",
    "                tifImage.convert('RGB').save(jpgImageFilePath + file.split('.')[0] + '.jpeg', 'JPEG')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read All jpg files to create data frame\n",
    "def ConvertJpegToDF(jpgImageFilePath):\n",
    "    \n",
    "    tiffDF = pd.DataFrame({'Complaint_Type': [], 'Consumer_Complaint_Description': []})\n",
    "    complaintType = []\n",
    "    complaintDescription = []\n",
    "    for r, d, f in os.walk(jpgImageFilePath):\n",
    "        for file in f:\n",
    "            if ('.jpeg' in str.lower(file)):\n",
    "                jpgImage = Image.open(jpgImageFilePath + file)\n",
    "                complaintDesciption = FilterEmailContent(jpgImage)\n",
    "                fileCategory = FileCategory(file)\n",
    "                complaintType.append(fileCategory)\n",
    "                complaintDescription.append(complaintDesciption)\n",
    "                \n",
    "    tiffDF['Complaint_Type'] = complaintType\n",
    "    tiffDF['Consumer_Complaint_Description'] = complaintDescription\n",
    "    RemoveAllJpegs(jpgImageFilePath)\n",
    "    return tiffDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the customer complaint email content\n",
    "def FilterEmailContent(jpgImage):\n",
    "    emailContent = pytesseract.image_to_string(jpgImage)\n",
    "    emailContent1 = pytesseract.image_to_data(jpgImage)\n",
    "    startBodyKeywords = ['Subject', 'Dear', 'Hello', 'Hi',]\n",
    "    endBodyKeywords = ['Thank you','Thanks & Regards', 'Sincerely']\n",
    "    for keyword in startBodyKeywords:\n",
    "        startIndex = emailContent.find(keyword)\n",
    "        if startIndex >= 0:\n",
    "            startIndex = startIndex + len(keyword) + 1\n",
    "            break\n",
    "        \n",
    "    for keyword in endBodyKeywords:\n",
    "        endIndex =  emailContent.find(keyword)\n",
    "        if endIndex >= 0:\n",
    "            #endIndex += len(keyword)\n",
    "            break\n",
    "            \n",
    "    if not (startIndex >= 0):\n",
    "        startIndex = 0\n",
    "    if not (endIndex >= 0):\n",
    "        endIndex = -1\n",
    "        \n",
    "    emailContent = emailContent[startIndex : endIndex]\n",
    "    \n",
    "    #Remove any email address or Phone Number\n",
    "    phoneIndex = emailContent.find('Phone:')\n",
    "    if (phoneIndex > 0):\n",
    "        emailContent = emailContent[0:phoneIndex] + emailContent[phoneIndex + 19 : -1]\n",
    "    \n",
    "    mailIndex = emailContent.find('Mail:')\n",
    "    mailIndex1 = emailContent.find('.com')\n",
    "    if ((mailIndex > 0) & (mailIndex1 > 0)):\n",
    "        emailContent = emailContent[0:mailIndex] + emailContent[mailIndex1 + 4 : -1]\n",
    "        \n",
    "    return emailContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the jpgs once content is read\n",
    "def RemoveAllJpegs(imagePath):\n",
    "    for r, d, f in os.walk(imagePath):\n",
    "        for file in f:\n",
    "            if (('.jpeg' in str.lower(file)) |('.jpg' in str.lower(file))):\n",
    "                os.remove(os.path.join(r, file))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the file category\n",
    "def FileCategory(fileName):\n",
    "    fileSections = fileName.split(' ')\n",
    "    if (len(fileSections) > 1):\n",
    "        fileCategory = fileSections[0] + ' ' + fileSections[1][:-6]\n",
    "    else:\n",
    "        fileSections = fileName.split('_')\n",
    "        fileCategory = fileSections[0] + ' ' + fileSections[1]\n",
    "    return fileCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStopWords(emails):\n",
    "    nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
    "    final_email_content = ''\n",
    "    listOfContent = []\n",
    "    for emailContent in emails:\n",
    "        emailContent = \"\".join([w for w in emailContent if w not in string.punctuation])\n",
    "        tokens = nltk.tokenize.word_tokenize(emailContent)\n",
    "        content = [token for token in tokens if not token in nltk_stopwords]\n",
    "        final_email_content = ' '.join(content)\n",
    "        listOfContent.append(final_email_content)\n",
    "        \n",
    "    return listOfContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing preprocessing on the data\n",
    "def PreprocessData(tiffDF):\n",
    "    processedTiffDF = tiffDF\n",
    "    \n",
    "    cols = ['Complaint_Type', 'Consumer_Complaint_Description']\n",
    "    df = tiffDF[cols]\n",
    "    \n",
    "    #Remove any null values from data on columns\n",
    "    df = df.dropna(subset=['Consumer_Complaint_Description','Complaint_Type'], how='any')\n",
    "        \n",
    "    #Do column encoding for better catogorization  \n",
    "    df['Complaint_Id'] = df['Complaint_Type'].factorize()[0]\n",
    "    \n",
    "    #Remove all new line characters\n",
    "    df['Consumer_Complaint_Description'] = df['Consumer_Complaint_Description'].replace('\\n', ' ', regex = True)\n",
    "    \n",
    "    #Remove all new line characters & puntuation\n",
    "    df['Consumer_Complaint_Description'] = RemoveStopWords(df['Consumer_Complaint_Description'])\n",
    "    \n",
    "    #Check imbalanced classes\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    df.groupby('Complaint_Id').Consumer_Complaint_Description.count().plot.bar(ylim=0)\n",
    "    plt.show()\n",
    "\n",
    "    #Remove imbalanced classes if required\n",
    "    \n",
    "    processedTiffDF = df\n",
    "    return processedTiffDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start vectorization \n",
    "def Vectorization(processedTiffDF):\n",
    "    tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "    features = tfidf.fit_transform(processedTiffDF.Consumer_Complaint_Description).toarray()\n",
    "    labels = processedTiffDF.Complaint_Id\n",
    "    return features, labels, tfidf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start TD-IDF Representation\n",
    "def TFIDFRep(processedDF, features, labels, tfidf):\n",
    "    N = 2\n",
    "    complaint_id_df = processedDF[['Complaint_Type', 'Complaint_Id']].drop_duplicates().sort_values('Complaint_Id')\n",
    "    complaint_to_id = dict(complaint_id_df.values)\n",
    "    for Complaint_Type, Complaint_Id in sorted(complaint_to_id.items()):\n",
    "        features_chi2 = chi2(features, labels == Complaint_Id)\n",
    "        indices = np.argsort(features_chi2[0])\n",
    "        feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "        unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "        bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "        print(\"# '{}':\".format(Complaint_Type))\n",
    "        print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "        print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model params to train model\n",
    "def GetModelParams(processedDF):\n",
    "    X_train = processedDF['Consumer_Complaint_Description']\n",
    "    y_train = processedDF['Complaint_Type']\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(X_train)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    return X_train_tfidf, y_train, count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algo1 - Naive Bayes Classifier\n",
    "def NaiveBayesClassifier(processedDF):\n",
    "    X_train_tfidf, y_train, count_vect = GetModelParams(processedDF)\n",
    "    clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "    return clf, count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algo2 - Support Vector Classification\n",
    "def LinearSupportVectorClassification(processedDF):\n",
    "    model = LinearSVC(random_state=0)\n",
    "    X_train_tfidf, y_train, count_vect = GetModelParams(processedDF)\n",
    "    model.fit(X_train_tfidf, y_train)    \n",
    "    return model, count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algo3 - Logistic Regression\n",
    "def SKLogisticRegression(processedDF):\n",
    "    model = LogisticRegression(random_state=101, solver='lbfgs',multi_class='multinomial')\n",
    "    X_train_tfidf, y_train, count_vect = GetModelParams(processedDF)\n",
    "    model.fit(X_train_tfidf, y_train)    \n",
    "    return model, count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algo4 - Random Forest\n",
    "def SKRandomForest(processedDF):\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=101)\n",
    "    X_train_tfidf, y_train, count_vect = GetModelParams(processedDF)\n",
    "    model.fit(X_train_tfidf, y_train)    \n",
    "    return model, count_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Pre-Training data for training\n",
    "ConvertTifToJpeg(tifImageFilePath, jpgImageFilePath)\n",
    "df = ConvertJpegToDF(jpgImageFilePath)\n",
    "processedDF = PreprocessData(df)\n",
    "#features,labels,tfidf = Vectorization(processedDF)\n",
    "#Represention words in vector for each category for uni-grams & bi-grams\n",
    "#TFIDFRep(processedDF, features, labels, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Pre-Testing data for testing\n",
    "ConvertTifToJpeg(tifTestImageFilePath, jpgTestImageFilePath)\n",
    "testDF = ConvertJpegToDF(jpgTestImageFilePath)\n",
    "\n",
    "#Clean the Data\n",
    "#remove all new line characters\n",
    "testDF['Consumer_Complaint_Description'] = testDF['Consumer_Complaint_Description'].replace('\\n', ' ', regex = True)\n",
    "    \n",
    "#remove all stop words & Puntuation\n",
    "testDF['Consumer_Complaint_Description'] = RemoveStopWords(testDF['Consumer_Complaint_Description'])\n",
    "\n",
    "y_test = []\n",
    "for index, row in testDF.iterrows():\n",
    "    #print('Complaint Type: ' + row['Complaint_Type'])\n",
    "    y_test.append(row['Complaint_Type'])\n",
    "    #stringVal = '\"' + row['Consumer_Complaint_Description'] + '\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions from Naive Bayes\n",
    "y_pred = []\n",
    "NBClassifier, count_vect = NaiveBayesClassifier(processedDF)\n",
    "y_pred = list(NBClassifier.predict(count_vect.transform(testDF['Consumer_Complaint_Description'])))\n",
    "print(\"Accuracy Score: \")\n",
    "print(accuracy_score(y_test, y_pred) * 100)\n",
    "confidence_Score = NBClassifier.predict_proba(count_vect.transform(testDF['Consumer_Complaint_Description']))\n",
    "\n",
    "for prediction, cScores, in zip(y_pred, confidence_Score):\n",
    "    print('Prediction: ' + prediction + '\\nConfidence Score: ' + str(max(cScores) * 100) + '%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions from Support Vector Machine\n",
    "SVCClassifier, count_vect = LinearSupportVectorClassification(processedDF)\n",
    "y_pred = SVCClassifier.predict(count_vect.transform(testDF['Consumer_Complaint_Description']))\n",
    "print('Predictions: ')\n",
    "print(y_pred)\n",
    "print(\"Accuracy Score: \")\n",
    "print(accuracy_score(y_test, y_pred) * 100)\n",
    "confidence_Score = NBClassifier.predict_proba(count_vect.transform(testDF['Consumer_Complaint_Description']))\n",
    "\n",
    "for prediction, cScores, in zip(y_pred, confidence_Score):\n",
    "    print('Prediction: ' + prediction + '\\nConfidence Score: ' + str(max(cScores) * 100) + '%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions from LogisticRegression\n",
    "LogisticRegressionClassifier, count_vect = SKLogisticRegression(processedDF)\n",
    "y_pred = LogisticRegressionClassifier.predict(count_vect.transform(testDF['Consumer_Complaint_Description']))\n",
    "print('Predictions: ')\n",
    "print(y_pred)\n",
    "print(\"Accuracy Score: \")\n",
    "print(accuracy_score(y_test, y_pred) * 100)\n",
    "confidence_Score = NBClassifier.predict_proba(count_vect.transform(testDF['Consumer_Complaint_Description']))\n",
    "\n",
    "for prediction, cScores, in zip(y_pred, confidence_Score):\n",
    "    print('Prediction: ' + prediction + '\\nConfidence Score: ' + str(max(cScores) * 100) + '%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions from Random Forest\n",
    "RandomForestClassifier, count_vect = SKRandomForest(processedDF)\n",
    "y_pred = RandomForestClassifier.predict(count_vect.transform(testDF['Consumer_Complaint_Description']))\n",
    "print('Predictions: ')\n",
    "print(y_pred)\n",
    "print(\"Accuracy Score: \")\n",
    "print(accuracy_score(y_test, y_pred) * 100)\n",
    "confidence_Score = NBClassifier.predict_proba(count_vect.transform(testDF['Consumer_Complaint_Description']))\n",
    "\n",
    "for prediction, cScores, in zip(y_pred, confidence_Score):\n",
    "    print('Prediction: ' + prediction + '\\nConfidence Score: ' + str(max(cScores) * 100) + '%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish the full metrics report \n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=df['Complaint_Type'].unique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
